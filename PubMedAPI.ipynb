{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading stopwords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:git.cmd:Popen(['git', 'version'], cwd=/Users/aravind/Documents/LAIR Hub/CODE_v2/pattie_humanities, stdin=None, shell=False, universal_newlines=False)\n",
      "DEBUG:git.cmd:Popen(['git', 'version'], cwd=/Users/aravind/Documents/LAIR Hub/CODE_v2/pattie_humanities, stdin=None, shell=False, universal_newlines=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aravind/Documents/LAIR Hub/CODE_v2/pattie_humanities\n"
     ]
    }
   ],
   "source": [
    "from flask_login import current_user\n",
    "import visual_library_plos as vl\n",
    "import simplejson\n",
    "from urllib.parse import urlencode\n",
    "from urllib.request import urlopen\n",
    "from io import StringIO\n",
    "from contextlib import closing\n",
    "from flask_session import Session\n",
    "from flask import (\n",
    "    render_template,\n",
    "    request,\n",
    "    redirect,\n",
    "    url_for,\n",
    "    send_from_directory,\n",
    "    g,\n",
    "    flash,\n",
    "    session,\n",
    "    jsonify,\n",
    "    Response,\n",
    "    json,\n",
    ")\n",
    "from sklearn import preprocessing\n",
    "from scipy.sparse.linalg import norm\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from app import app, tokenize, exclude, stopwords, MINDF\n",
    "from db.db_get_data import get_dc_data\n",
    "\n",
    "# import argparse\n",
    "# import sys\n",
    "# import operator\n",
    "import math\n",
    "\n",
    "# import gzip\n",
    "# import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "time.clock = time.time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "import pysolr\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aravind/Documents/LAIR Hub/CODE_v2/pattie_humanities\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(raw_query_inp, num_cls_inp):\n",
    "    print(\"cluster\")\n",
    "    error = None\n",
    "    dataset = \"PubMedAPI\"\n",
    "\n",
    "    field = \"All fields\"\n",
    "    raw_query = raw_query_inp\n",
    "    entity = \"keywords\"\n",
    "    q = raw_query\n",
    "    # print(q)\n",
    "    num_cls = num_cls_inp\n",
    "\n",
    "    if dataset == \"Placeholder\":\n",
    "        return redirect(url_for(\"home\"))\n",
    "    \n",
    "    # dynamic dataset\n",
    "    elif dataset == \"PubMedAPI\":\n",
    "        print(\"PubMedAPI in routes.py\")\n",
    "        import cluster_pymedAPI\n",
    "        import es_search\n",
    "\n",
    "        # print(\"start_date\")\n",
    "        # print(start_date)\n",
    "        num_cls = int(num_cls)\n",
    "        # try:\n",
    "        # data, error = es_search.retrieve(q, entity)\n",
    "        data, error = cluster_pymedAPI.retrieve(q)\n",
    "        # data, error = get_dc_data(q)\n",
    "        # http://localhost:5000/cluster?dataset_opt=PubMedAPI&query=Javed Mostafa\n",
    "        # http://localhost:5000/cluster?dataset_opt=PLOS&query=digital%20health\n",
    "        # http://localhost:5000/cluster?dataset_opt=NewsAPI&query=digital%20health\n",
    "        print(data, error)\n",
    "        print(type(data), data.columns.tolist())\n",
    "        # except:\n",
    "        #     flash(\"The National Library of Medicine Entrez API is experiencing technical issues. We're sorry for this inconvenience.\")\n",
    "        #     return redirect(url_for('home'))\n",
    "\n",
    "        if error == None:\n",
    "            if entity == \"genes\":\n",
    "                doc_term_mat, df, w2id, bibs = vl.read_df_genes(\n",
    "                    session[\"gene_set\"], data, dataset, stopwords\n",
    "                )\n",
    "                # print('-----------df------------')\n",
    "                # print(df)\n",
    "            elif entity == \"authors\":\n",
    "                # doc_term_mat, df, w2id, bibs = vl.read_df_authors(\n",
    "                #     data, dataset, stopwords)\n",
    "                (\n",
    "                    orig_doc_term_mat,\n",
    "                    orig_df,\n",
    "                    df,\n",
    "                    dfr,\n",
    "                    keywords,\n",
    "                    cluster_centers,\n",
    "                    cluster_desc,\n",
    "                    coordinates,\n",
    "                    id2members,\n",
    "                    bibs,\n",
    "                    org_ids,\n",
    "                    author_names,\n",
    "                    author_docs,\n",
    "                ) = vl.process_authors_4(data, dataset, stopwords)\n",
    "            else:\n",
    "                doc_term_mat, df, w2id, bibs = vl.read_df(data, dataset, stopwords)\n",
    "        else:\n",
    "            # flash(error)\n",
    "            return redirect(url_for(\"home\"))\n",
    "\n",
    "    # uploaded dataset\n",
    "    else:\n",
    "        # num_cls = 10\n",
    "        uploaded_file = dataset\n",
    "        # path = UPLOAD_FOLDER  # needs to be changed\n",
    "        # print(path)\n",
    "        doc_term_mat, df, w2id, bibs = vl.read_plaintext(\n",
    "            path, uploaded_file, q, dataset, stopwords\n",
    "        )\n",
    "\n",
    "    # print(\"doc_term_mat\")\n",
    "    # print(doc_term_mat)\n",
    "    # print(df)\n",
    "\n",
    "    if dataset != \"Experts\" and len(bibs) < 10:\n",
    "        #       flash('No clusters found. Broaden your search.')\n",
    "        flash(\n",
    "            \"Please upload larger files or search for more relevant keywords to get enough results.\"\n",
    "        )\n",
    "        #        return redirect(url_for('home'))\n",
    "        return redirect(request.referrer)\n",
    "\n",
    "    if entity != \"authors\" and dataset != \"Experts\":\n",
    "        # Remove terms whose df is lower than mindf\n",
    "        if MINDF > 0:\n",
    "            inf = []\n",
    "            for w in df:\n",
    "                if df[w] <= MINDF:\n",
    "                    inf.append(w)\n",
    "            for w in inf:\n",
    "                del df[w]\n",
    "\n",
    "        # Save org data\n",
    "        orig_doc_term_mat = doc_term_mat\n",
    "        orig_df = df\n",
    "\n",
    "        # Compute tfidf and find key terms\n",
    "        # print(\"Computing TFIDF and finding key terms...\")\n",
    "        if dataset == \"NYTIMES\" or dataset == \"PLOS\" or dataset == \"DIABETES\":\n",
    "            doc_term_mat, dfr = vl.compute_tfidf(doc_term_mat, df, rank=10)\n",
    "        else:\n",
    "            doc_term_mat, dfr = vl.compute_tfidf(doc_term_mat, df, rank=30)\n",
    "\n",
    "        # Sort and output results (discovered keywords)\n",
    "        keywords = vl.output_keywords(len(doc_term_mat), dfr, df, p_docs=1.0)\n",
    "        # print('Keywords...')\n",
    "        # print(keywords)\n",
    "\n",
    "        # Create new matrix with the keywords\n",
    "        doc_term_mat, org_ids = vl.update(doc_term_mat, keywords)\n",
    "\n",
    "        # Convert to sparse matrix\n",
    "        doc_term_mat = vl.convert_sparse(doc_term_mat, keywords)\n",
    "\n",
    "        # Clustering\n",
    "        # print()\n",
    "        # print(\"Clustering...\")\n",
    "\n",
    "        # n_components: number of dimensions for LSA\n",
    "        # k: number of clusters\n",
    "        # n_desc: number of keywords (desc) for each cluster\n",
    "\n",
    "        if dataset == \"PLOS\":\n",
    "            id2members, cluster_centers, cluster_desc, coordinates, error = vl.kmeans(\n",
    "                doc_term_mat, keywords, org_ids, n_components=50, k=num_cls, n_desc=25\n",
    "            )\n",
    "        else:\n",
    "            id2members, cluster_centers, cluster_desc, coordinates, error = vl.kmeans(\n",
    "                doc_term_mat, keywords, org_ids, n_components=20, k=num_cls, n_desc=15\n",
    "            )\n",
    "\n",
    "        if error != None:  # needs to be changed\n",
    "            print(error)\n",
    "            flash(\n",
    "                \"Sorry, the entity you've chosen does not generate meaningful clusters for current query. Please try other entities or queries. \"\n",
    "            )\n",
    "            return redirect(request.referrer)\n",
    "\n",
    "    # get cluster colors\n",
    "    \"\"\"\n",
    "    # center is (.5,0)\n",
    "    cc = np.array(cluster_centers)-[.5,0]\n",
    "    hue = ((np.arctan2(cc[:,1],cc[:,0])/np.pi+1)*180*2).\\\n",
    "        astype(int).tolist()\n",
    "    satr = (np.linalg.norm(cc, axis=1)/math.sqrt(5)*2).tolist()\n",
    "    \"\"\"\n",
    "    # center is (.5,.5)\n",
    "    cc = np.array(cluster_centers) - 0.5\n",
    "    hue = ((np.arctan2(cc[:, 1], cc[:, 0]) / np.pi + 1) * 180).astype(int).tolist()\n",
    "    satr = (np.linalg.norm(cc, axis=1) / math.sqrt(2) * 2).tolist()\n",
    "    val = [0.8] * num_cls\n",
    "\n",
    "    # Create data to pass to result.html\n",
    "    df_new = dict()\n",
    "    dfr_new = dict()\n",
    "    for w in keywords:\n",
    "        df_new[w] = df[w]\n",
    "        dfr_new[w] = dfr[w]\n",
    "\n",
    "    # create adjacency matrix that will be used for network edges\n",
    "    centroid_matrix = pd.DataFrame.from_records(cluster_centers)\n",
    "    centroid_distances = pd.DataFrame(\n",
    "        squareform(pdist(centroid_matrix, metric=\"euclidean\")),\n",
    "        columns=list(id2members.keys()),\n",
    "        index=list(id2members.keys()),\n",
    "    ).to_dict()\n",
    "\n",
    "    id2freq = {x: len(id2members[x]) for x in id2members}\n",
    "\n",
    "    # create network data structure\n",
    "    # print('\\nEUCLIDEAN DISTANCES\\n----------')\n",
    "    edges = []\n",
    "    for i in id2members:\n",
    "        for k, v in centroid_distances[i].items():\n",
    "            # print(i, ' to ', k, ' = ', v)\n",
    "            if i != k:\n",
    "                if v > 0.75:\n",
    "                    v = \"Distant\"\n",
    "                    edge = {\n",
    "                        \"clusterID\": i,\n",
    "                        \"source\": cluster_centers[i],\n",
    "                        \"target\": cluster_centers[k],\n",
    "                        \"distance\": v,\n",
    "                    }\n",
    "                    edges.append(edge)\n",
    "                elif v <= 0.75 and v > 0.50:\n",
    "                    v = \"Similar\"\n",
    "                    edge = {\n",
    "                        \"clusterID\": i,\n",
    "                        \"source\": cluster_centers[i],\n",
    "                        \"target\": cluster_centers[k],\n",
    "                        \"distance\": v,\n",
    "                    }\n",
    "                    edges.append(edge)\n",
    "                elif v <= 0.50:\n",
    "                    v = \"Very Similar\"\n",
    "                    edge = {\n",
    "                        \"clusterID\": i,\n",
    "                        \"source\": cluster_centers[i],\n",
    "                        \"target\": cluster_centers[k],\n",
    "                        \"distance\": v,\n",
    "                    }\n",
    "                    edges.append(edge)\n",
    "\n",
    "                # elif v >= 0.50:\n",
    "                #     v = \"Not Similar\"\n",
    "                # edge = {\"clusterID\":i,\"source\":cluster_centers[i],\"target\":cluster_centers[k],\"distance\":v}\n",
    "                # edges.append(edge)\n",
    "    # print('\\nBIBLIOGRAPHY\\n----------')\n",
    "    sources = []\n",
    "    id2meshTerms = {}\n",
    "    if dataset == \"PubMedAPI\":\n",
    "        from collections import Counter\n",
    "\n",
    "        for cluster_id in sorted(id2members.keys()):\n",
    "            concepts = cluster_desc[cluster_id]\n",
    "            references = []\n",
    "            for bib_id in id2members[cluster_id]:\n",
    "                references.append(bibs[bib_id])\n",
    "            for paper in references:\n",
    "                if type(paper[\"author\"]) != str and None in paper[\"author\"]:\n",
    "                    paper[\"author\"] = [i for i in paper[\"author\"] if i]\n",
    "            bibliography = {\"concepts\": concepts, \"references\": references}\n",
    "            meshTerms = []\n",
    "            for ref in references:\n",
    "                meshHeadingList = ref.get(\"meshHeadings\")  # ref[\"meshHeadings\"]\n",
    "                for mesh in meshHeadingList:\n",
    "                    meshTerms.append(mesh[\"descriptor\"])\n",
    "            # print('concept: {}'.format(concepts))\n",
    "            # print('meshTerms: {}'.format(Counter(meshTerms)))\n",
    "            # print('\\n')\n",
    "            id2meshTerms[cluster_id] = str(dict(Counter(meshTerms).most_common(20)))\n",
    "            # vl.genes_coverage(concepts, references)\n",
    "            sources.append(bibliography)\n",
    "        # print(sources)\n",
    "\n",
    "    \"\"\"\n",
    "    cnt = np.unique(membership, return_counts=True)\n",
    "    keys = [x for x in cnt[0]]\n",
    "    values = [int(x) for x in cnt[1]]\n",
    "    id2freq = dict(zip(keys, values))\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # Prepare bib data if num of documents is small\n",
    "    if len(org_ids) > 150:\n",
    "        bibs = []\n",
    "    \"\"\"\n",
    "\n",
    "    # session['doc_term_mat'] = doc_term_mat  # term-doc matrix\n",
    "    session = {}\n",
    "    session[\"docs_org\"] = orig_doc_term_mat  # doc-term raw freq matrix\n",
    "    session[\"df_org\"] = orig_df\n",
    "    session[\"num_cls\"] = num_cls\n",
    "    session[\"id2members_0\"] = id2members\n",
    "    session[\"cluster_desc_0\"] = cluster_desc\n",
    "    session[\"xy_0\"] = cluster_centers\n",
    "    # session[\"state\"] = state\n",
    "    session[\"hue_0\"] = hue\n",
    "    session[\"satr_0\"] = satr\n",
    "    session[\"org_ids_0\"] = org_ids\n",
    "    session[\"bibs_0\"] = bibs\n",
    "    session[\"dataset\"] = dataset\n",
    "    session[\"edges_0\"] = edges\n",
    "    session[\"sources\"] = sources\n",
    "    session[\"id2meshTerms\"] = id2meshTerms\n",
    "\n",
    "    session[\"entity\"] = entity\n",
    "    if entity == \"authors\":\n",
    "        session[\"author_names\"] = author_names\n",
    "        session[\"author_docs\"] = author_docs\n",
    "        \n",
    "    # Convert NumPy arrays to lists\n",
    "    def convert_ndarray_to_list(obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_ndarray_to_list(item) for item in obj]\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: convert_ndarray_to_list(value) for key, value in obj.items()}\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    # Ensure orig_doc_term_mat is a list\n",
    "    orig_doc_term_mat_list = convert_ndarray_to_list(orig_doc_term_mat)\n",
    "    # Ensure orig_df is a list or dict (depending on its original type)\n",
    "    orig_df_list = convert_ndarray_to_list(orig_df)\n",
    "    # Ensure cluster_centers is a list\n",
    "    cluster_centers_list = convert_ndarray_to_list(cluster_centers)\n",
    "    # Ensure coordinates is a list\n",
    "    coordinates_list = convert_ndarray_to_list(coordinates)\n",
    "\n",
    "\n",
    "    sessionData = {\n",
    "        \"docs_org\": convert_ndarray_to_list(orig_doc_term_mat),  # doc-term raw freq matrix\n",
    "        \"df_org\": convert_ndarray_to_list(orig_df),\n",
    "        \"num_cls\": num_cls,\n",
    "        \"id2members_0\": convert_ndarray_to_list(id2members),\n",
    "        \"cluster_desc_0\": convert_ndarray_to_list(cluster_desc),\n",
    "        \"xy_0\": convert_ndarray_to_list(cluster_centers),\n",
    "        # \"state\": state,\n",
    "        \"hue_0\": convert_ndarray_to_list(hue),\n",
    "        \"satr_0\": convert_ndarray_to_list(satr),\n",
    "        \"org_ids_0\": convert_ndarray_to_list(org_ids),\n",
    "        \"dataset\": dataset,\n",
    "        \"edges_0\": convert_ndarray_to_list(edges),\n",
    "        \"sources\": convert_ndarray_to_list(sources)\n",
    "    }\n",
    "\n",
    "    # session['df'] = df_new\n",
    "    # session['dfr'] = dfr_new\n",
    "    # session['keywords'] = keywords\n",
    "\n",
    "    # print(\"bibs\")\n",
    "    # print(bibs)\n",
    "\n",
    "    # output the data to terminal\n",
    "    # print('\\nCENTROIDS\\n----------')\n",
    "    # print(cluster_centers)\n",
    "    # print('\\nCENTROID DISTANCES\\n----------')\n",
    "    # print(centroid_distances)\n",
    "    # print('\\nCLUSTER LABELS\\n----------')\n",
    "    # print(cluster_desc)\n",
    "    # print('\\nCLUSTER ID & PUBLICATIONS\\n----------')\n",
    "    # print(id2members)\n",
    "    # print('\\nCLUSTER ID & PUBLICATION COUNT\\n----------')\n",
    "    # print(id2freq)\n",
    "    # print('\\nNETWORK\\n----------')\n",
    "    # print(edges)\n",
    "\n",
    "\n",
    "\n",
    "    if entity == \"authors\" or entity == \"experts\" or entity == \"topics\":\n",
    "        # to tell the difference between keyword search and author search so that sessionStorage won't keep old data when switching between them\n",
    "        q = q + \"_\" + entity\n",
    "\n",
    "    print(\"exit cluster\")\n",
    "    return {\n",
    "        \"sessionData\": sessionData,\n",
    "        \"raw_query\": raw_query,\n",
    "        \"q\": q,\n",
    "        \"error\": error,\n",
    "        \"keywords\": keywords,\n",
    "        \"df\": df_new,\n",
    "        \"dfr\": dfr_new,\n",
    "        \"cluster_desc\": cluster_desc,\n",
    "        \"xy\": cluster_centers_list,\n",
    "        \"edges\": edges,\n",
    "        \"id2freq\": id2freq,\n",
    "        \"xy_inter\": coordinates_list,\n",
    "        \"hue\": hue,\n",
    "        \"satr\": satr,\n",
    "        \"id2members\": id2members,\n",
    "        \"sources\": sources,\n",
    "        \"dataset\": dataset,\n",
    "        \"num_cls\": num_cls,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
      "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
      "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
      "DEBUG:h5py._conv:Creating converter from 5 to 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster\n",
      "PubMedAPI in routes.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): eutils.ncbi.nlm.nih.gov:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving data from PubMed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/aravind/Documents/LAIR Hub/CODE_v2/pattie_humanities/cluster_pymedAPI.py\", line 35, in retrieve\n",
      "    results = pubmed.query(query, max_results=300)\n",
      "  File \"/Users/aravind/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/pymed/api.py\", line 62, in query\n",
      "    article_ids = self._getArticleIds(query=query, max_results=max_results)\n",
      "  File \"/Users/aravind/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/pymed/api.py\", line 208, in _getArticleIds\n",
      "    response = self._get(url=\"/entrez/eutils/esearch.fcgi\", parameters=parameters)\n",
      "  File \"/Users/aravind/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/pymed/api.py\", line 140, in _get\n",
      "    response = requests.get(f\"{BASE_URL}{url}\", params=parameters)\n",
      "  File \"/Users/aravind/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/requests/api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"/Users/aravind/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/aravind/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/aravind/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/aravind/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/aravind/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"/Users/aravind/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/Users/aravind/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 1095, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/Users/aravind/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/urllib3/connection.py\", line 615, in connect\n",
      "    self.sock = sock = self._new_conn()\n",
      "  File \"/Users/aravind/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/urllib3/connection.py\", line 196, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "  File \"/Users/aravind/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/urllib3/util/connection.py\", line 60, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/Users/aravind/anaconda3/envs/PATTIE_v2/lib/python3.10/socket.py\", line 955, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error:  None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/aravind/Documents/LAIR Hub/CODE_v2/pattie_humanities/PubMedAPI.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aravind/Documents/LAIR%20Hub/CODE_v2/pattie_humanities/PubMedAPI.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cluster(\u001b[39m\"\u001b[39;49m\u001b[39mFei Yu\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/Users/aravind/Documents/LAIR Hub/CODE_v2/pattie_humanities/PubMedAPI.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aravind/Documents/LAIR%20Hub/CODE_v2/pattie_humanities/PubMedAPI.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m num_cls \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(num_cls)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aravind/Documents/LAIR%20Hub/CODE_v2/pattie_humanities/PubMedAPI.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# try:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aravind/Documents/LAIR%20Hub/CODE_v2/pattie_humanities/PubMedAPI.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# data, error = es_search.retrieve(q, entity)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aravind/Documents/LAIR%20Hub/CODE_v2/pattie_humanities/PubMedAPI.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m data, error \u001b[39m=\u001b[39m cluster_pymedAPI\u001b[39m.\u001b[39;49mretrieve(q)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aravind/Documents/LAIR%20Hub/CODE_v2/pattie_humanities/PubMedAPI.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# data, error = get_dc_data(q)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aravind/Documents/LAIR%20Hub/CODE_v2/pattie_humanities/PubMedAPI.ipynb#W2sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# http://localhost:5000/cluster?dataset_opt=PubMedAPI&query=Javed Mostafa\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aravind/Documents/LAIR%20Hub/CODE_v2/pattie_humanities/PubMedAPI.ipynb#W2sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# http://localhost:5000/cluster?dataset_opt=PLOS&query=digital%20health\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aravind/Documents/LAIR%20Hub/CODE_v2/pattie_humanities/PubMedAPI.ipynb#W2sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# http://localhost:5000/cluster?dataset_opt=NewsAPI&query=digital%20health\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aravind/Documents/LAIR%20Hub/CODE_v2/pattie_humanities/PubMedAPI.ipynb#W2sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(data, error)\n",
      "File \u001b[0;32m~/Documents/LAIR Hub/CODE_v2/pattie_humanities/cluster_pymedAPI.py:35\u001b[0m, in \u001b[0;36mretrieve\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[39m# query PubMed object with user input\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m         results \u001b[39m=\u001b[39m pubmed\u001b[39m.\u001b[39;49mquery(query, max_results\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m)\n\u001b[1;32m     36\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUnexpected error: \u001b[39m\u001b[39m\"\u001b[39m, traceback\u001b[39m.\u001b[39mprint_exc())\n",
      "File \u001b[0;32m~/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/pymed/api.py:62\u001b[0m, in \u001b[0;36mPubMed.query\u001b[0;34m(self, query, max_results)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Method that executes a query agains the GraphQL schema, automatically\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m    inserting the PubMed data loader.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39m                    in the \"data\" attribute.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m# Retrieve the article IDs for the query\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m article_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getArticleIds(query\u001b[39m=\u001b[39;49mquery, max_results\u001b[39m=\u001b[39;49mmax_results)\n\u001b[1;32m     64\u001b[0m \u001b[39m# Get the articles themselves\u001b[39;00m\n\u001b[1;32m     65\u001b[0m articles \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m     66\u001b[0m     [\n\u001b[1;32m     67\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getArticles(article_ids\u001b[39m=\u001b[39mbatch)\n\u001b[1;32m     68\u001b[0m         \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m batches(article_ids, \u001b[39m250\u001b[39m)\n\u001b[1;32m     69\u001b[0m     ]\n\u001b[1;32m     70\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/pymed/api.py:208\u001b[0m, in \u001b[0;36mPubMed._getArticleIds\u001b[0;34m(self, query, max_results)\u001b[0m\n\u001b[1;32m    205\u001b[0m     parameters[\u001b[39m\"\u001b[39m\u001b[39mretmax\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m max_results\n\u001b[1;32m    207\u001b[0m \u001b[39m# Make the first request to PubMed\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get(url\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/entrez/eutils/esearch.fcgi\u001b[39;49m\u001b[39m\"\u001b[39;49m, parameters\u001b[39m=\u001b[39;49mparameters)\n\u001b[1;32m    210\u001b[0m \u001b[39m# Add the retrieved IDs to the list\u001b[39;00m\n\u001b[1;32m    211\u001b[0m article_ids \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mesearchresult\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39midlist\u001b[39m\u001b[39m\"\u001b[39m, [])\n",
      "File \u001b[0;32m~/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/pymed/api.py:140\u001b[0m, in \u001b[0;36mPubMed._get\u001b[0;34m(self, url, parameters, output)\u001b[0m\n\u001b[1;32m    137\u001b[0m parameters[\u001b[39m\"\u001b[39m\u001b[39mretmode\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m output\n\u001b[1;32m    139\u001b[0m \u001b[39m# Make the request to PubMed\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mBASE_URL\u001b[39m}\u001b[39;49;00m\u001b[39m{\u001b[39;49;00murl\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, params\u001b[39m=\u001b[39;49mparameters)\n\u001b[1;32m    142\u001b[0m \u001b[39m# Check for any errors\u001b[39;00m\n\u001b[1;32m    143\u001b[0m response\u001b[39m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    670\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    671\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    672\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    673\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    674\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    675\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    676\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    677\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    678\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    467\u001b[0m     \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/urllib3/connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[39mif\u001b[39;00m conn\u001b[39m.\u001b[39mis_closed:\n\u001b[0;32m-> 1095\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1097\u001b[0m \u001b[39m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m~/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/urllib3/connection.py:615\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    614\u001b[0m     sock: socket\u001b[39m.\u001b[39msocket \u001b[39m|\u001b[39m ssl\u001b[39m.\u001b[39mSSLSocket\n\u001b[0;32m--> 615\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m sock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    616\u001b[0m     server_hostname: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[1;32m    617\u001b[0m     tls_in_tls \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/urllib3/connection.py:196\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[39m:return: New socket connection.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     sock \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    197\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport),\n\u001b[1;32m    198\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout,\n\u001b[1;32m    199\u001b[0m         source_address\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address,\n\u001b[1;32m    200\u001b[0m         socket_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket_options,\n\u001b[1;32m    201\u001b[0m     )\n\u001b[1;32m    202\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    203\u001b[0m     \u001b[39mraise\u001b[39;00m NameResolutionError(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/urllib3/util/connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mUnicodeError\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[39mraise\u001b[39;00m LocationParseError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mhost\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, label empty or too long\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, socket\u001b[39m.\u001b[39;49mSOCK_STREAM):\n\u001b[1;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n\u001b[1;32m     62\u001b[0m     sock \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/PATTIE_v2/lib/python3.10/socket.py:955\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[39m# We override this function since we want to translate the numeric family\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[39m# and socket type values to enum constants.\u001b[39;00m\n\u001b[1;32m    954\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 955\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[1;32m    956\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n\u001b[1;32m    957\u001b[0m     addrlist\u001b[39m.\u001b[39mappend((_intenum_converter(af, AddressFamily),\n\u001b[1;32m    958\u001b[0m                      _intenum_converter(socktype, SocketKind),\n\u001b[1;32m    959\u001b[0m                      proto, canonname, sa))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cluster(\"Fei Yu\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): eutils.ncbi.nlm.nih.gov:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster\n",
      "PubMedAPI in routes.py\n",
      "Retrieving data from PubMed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://eutils.ncbi.nlm.nih.gov:443 \"GET /entrez/eutils/esearch.fcgi?tool=PATTIE&email=nibras.rakib%40mail.utoronto.ca&db=pubmed&term=Fei+Yu&retmax=300&retmode=json HTTP/11\" 200 None\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): eutils.ncbi.nlm.nih.gov:443\n",
      "DEBUG:urllib3.connectionpool:https://eutils.ncbi.nlm.nih.gov:443 \"GET /entrez/eutils/efetch.fcgi?tool=PATTIE&email=nibras.rakib%40mail.utoronto.ca&db=pubmed&id=39024040&id=39018421&id=39015361&id=38992675&id=38990325&id=38984807&id=38981857&id=38978048&id=38977905&id=38975298&id=38969409&id=38953634&id=38950519&id=38949397&id=38942189&id=38941917&id=38925411&id=38914695&id=38905100&id=38901733&id=38897173&id=38890248&id=38889048&id=38878672&id=38878412&id=38875577&id=38871478&id=38860211&id=38858958&id=38858912&id=38858393&id=38852399&id=38849239&id=38839038&id=38828289&id=38827471&id=38822373&id=38820538&id=38817980&id=38813167&id=38802944&id=38789713&id=38782278&id=38781292&id=38766678&id=38757817&id=38719139&id=38717456&id=38703918&id=38703075&id=38694441&id=38693902&id=38670268&id=38667972&id=38660707&id=38650712&id=38649054&id=38580972&id=38571273&id=38571157&id=38570765&id=38553831&id=38551343&id=38549560&id=38542924&id=38540145&id=38535701&id=38517163&id=38507605&id=38503944&id=38495691&id=38493623&id=38475160&id=38471421&id=38462443&id=38444855&id=38443616&id=38443447&id=38441577&id=38436834&id=38429403&id=38391271&id=38390171&id=38384366&id=38377560&id=38377550&id=38368946&id=38351146&id=38335866&id=38325008&id=38321649&id=38311336&id=38307484&id=38307042&id=38303107&id=38289337&id=38283351&id=38272890&id=38269051&id=38245686&id=38228501&id=38226979&id=38200661&id=38190088&id=38186313&id=38169589&id=38157797&id=38155974&id=38141451&id=38132498&id=38126361&id=38111732&id=38106477&id=38092080&id=38091767&id=38063327&id=38057619&id=38054211&id=38051736&id=38040931&id=38034582&id=38032278&id=38016312&id=38001551&id=38001358&id=37997553&id=37944752&id=37935267&id=37929061&id=37921151&id=37916117&id=37914959&id=37898281&id=37893351&id=37891525&id=37868956&id=37867842&id=37858787&id=37856527&id=37845027&id=37839372&id=37820759&id=37795203&id=37788300&id=37778832&id=37778374&id=37777862&id=37767336&id=37767098&id=37745212&id=37713827&id=37713625&id=37712146&id=37711030&id=37710444&id=37699461&id=37695759&id=37689232&id=37685944&id=37680479&id=37679705&id=37672254&id=37668229&id=37654663&id=37649147&id=37637089&id=37634709&id=37633144&id=37619943&id=37610204&id=37574162&id=37567522&id=37565606&id=37559931&id=37520558&id=37516999&id=37505911&id=37505891&id=37495146&id=37475299&id=37464358&id=37455724&id=37453102&id=37438300&id=37435787&id=37409115&id=37392073&id=37381274&id=37371619&id=37363403&id=37354741&id=37315377&id=37311049&id=37306468&id=37299931&id=37282658&id=37277319&id=37277114&id=37272483&id=37267951&id=37258510&id=37251383&id=37229455&id=37221600&id=37211091&id=37208602&id=37206197&id=37204828&id=37204026&id=37200162&id=37195193&id=37187998&id=37180142&id=37177955&id=37170346&id=37165618&id=37155807&id=37146272&id=37142819&id=37130216&id=37126253&id=37103940&id=37090979&id=37085729&id=37062041&id=37061606&id=37056151&id=37046316&id=37043243&id=37039144&id=37033246&id=37015242&id=37006694&id=37005968&id=36994815&id=36989487&id=36984368&id=36949709&id=36946914&id=36946759&id=36946733&id=36944281&id=36942225&id=36935089&id=36931608&id=36931428&id=36925001&id=36922233&id=36917559&id=36903584&retmode=xml HTTP/11\" 200 None\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): eutils.ncbi.nlm.nih.gov:443\n",
      "DEBUG:urllib3.connectionpool:https://eutils.ncbi.nlm.nih.gov:443 \"GET /entrez/eutils/efetch.fcgi?tool=PATTIE&email=nibras.rakib%40mail.utoronto.ca&db=pubmed&id=36897674&id=36891150&id=36875475&id=36867218&id=36860060&id=36852460&id=36850086&id=36845314&id=36842967&id=36831718&id=36823877&id=36823329&id=36816047&id=36799131&id=36798927&id=36786817&id=36785899&id=36770966&id=36746285&id=36746261&id=36745151&id=36733920&id=36731322&id=36720201&id=36700261&id=36690610&id=36671448&id=36662717&id=36651361&id=36623538&id=36609338&id=36608570&id=36600222&id=36544682&id=36538724&id=36538092&id=36525243&id=36524627&id=36514714&id=36504782&id=36469521&id=38647873&id=36447752&id=36442852&id=36432276&id=36429832&id=36422328&id=36420413&id=36419650&id=36401465&retmode=xml HTTP/11\" 200 None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [Kai-Wen Tong, Fei Yu, Hao Wang, Kang Huang, Z...\n",
      "1      [Qian Niu, Tao-Yuan Yu, Jing-Wen Shi, Qing Hua...\n",
      "2      [Zhegang Zhou, Longbiao Yu, Fanbin Meng, Jingj...\n",
      "3      [Yang Ji, Chuangye Ni, Yanjun Shen, Zhenggang ...\n",
      "4      [Fei Yu, Mingguang Yang, Cheng He, Yanli Yang,...\n",
      "                             ...                        \n",
      "295    [Yao Cai, Fei Yu, Manish Kumar, Roderick Gladn...\n",
      "296    [Shumin Xie, Runyao Liu, Huiling Zhang, Fei Yu...\n",
      "297    [Bo Zhou, Haotian Qin, Yicong Huang, Quanzhen ...\n",
      "298    [Xiaoju Shen, Xiaocheng Mo, Weidan Tan, Xiaoxi...\n",
      "299     [Yewen Wang, Chunzhi Gong, Fei Yu, Quanyi Zhang]\n",
      "Name: authors, Length: 300, dtype: object <class 'pandas.core.series.Series'>\n",
      "Going to return df\n",
      "                                               authors  \\\n",
      "0    [Kai-Wen Tong, Fei Yu, Hao Wang, Kang Huang, Z...   \n",
      "1    [Qian Niu, Tao-Yuan Yu, Jing-Wen Shi, Qing Hua...   \n",
      "2    [Zhegang Zhou, Longbiao Yu, Fanbin Meng, Jingj...   \n",
      "3    [Yang Ji, Chuangye Ni, Yanjun Shen, Zhenggang ...   \n",
      "4    [Fei Yu, Mingguang Yang, Cheng He, Yanli Yang,...   \n",
      "..                                                 ...   \n",
      "295  [Yao Cai, Fei Yu, Manish Kumar, Roderick Gladn...   \n",
      "296  [Shumin Xie, Runyao Liu, Huiling Zhang, Fei Yu...   \n",
      "297  [Bo Zhou, Haotian Qin, Yicong Huang, Quanzhen ...   \n",
      "298  [Xiaoju Shen, Xiaocheng Mo, Weidan Tan, Xiaoxi...   \n",
      "299   [Yewen Wang, Chunzhi Gong, Fei Yu, Quanyi Zhang]   \n",
      "\n",
      "                                          affiliations  \\\n",
      "0    [State Key Laboratory of Geomechanics and Geot...   \n",
      "1    [Jiangsu Collaborative Innovation Centre of Bi...   \n",
      "2    [From the Department of Hand and Microsurgery,...   \n",
      "3    [Hepatobiliary Center, The First Affiliated Ho...   \n",
      "4    [Department of Radiology, Affiliated Hospital ...   \n",
      "..                                                 ...   \n",
      "295  [School of Information and Library Science, Th...   \n",
      "296  [Jiangsu Key Laboratory of Marine Bioresources...   \n",
      "297  [Department of Hand & Microsurgery, Peking Uni...   \n",
      "298  [Department of Pharmacology, School of Pharmac...   \n",
      "299  [Department of Anesthesiology, Binzhou Medical...   \n",
      "\n",
      "                                                 title  \\\n",
      "0    Microscale Normal Compression Behaviors of Cla...   \n",
      "1    Constructing Functional Radiation-Resistant Th...   \n",
      "2    Limb Salvage Using Lateral Circumflex Femoral ...   \n",
      "3    ESRP1-mediated biogenesis of circPTPN12 inhibi...   \n",
      "4    CT radiomics combined with clinical and radiol...   \n",
      "..                                                 ...   \n",
      "295  Health Recommender Systems Development, Usage,...   \n",
      "296                       Comparative Analyses of the    \n",
      "297  Atrophie blanche complicated with lower limb i...   \n",
      "298  KIAA1199 Correlates With Tumor Microenvironmen...   \n",
      "299  Effect of dexmedetomidine on intrapulmonary sh...   \n",
      "\n",
      "                                              abstract      pmid  \\\n",
      "0    Given the limitations of micromechanical exper...  39024040   \n",
      "1    When catalytic reactions are interfered with b...  39018421   \n",
      "2    A 50-year-old man was admitted to the hospital...  39015361   \n",
      "3    Emerging evidence indicates the pivotal involv...  38992675   \n",
      "4    This study aimed to establish a hematoma expan...  38990325   \n",
      "..                                                 ...       ...   \n",
      "295  A health recommender system (HRS) provides a u...  36429832   \n",
      "296  The change in life activities throughout a cyc...  36422328   \n",
      "297  Atrophie blanche (AB) is a thrombotic vascular...  36420413   \n",
      "298                                                     36419650   \n",
      "299  The effects of dexmedetomidine on the circulat...  36401465   \n",
      "\n",
      "                                                   url pubYear     pubdate  \\\n",
      "0    https://www.ncbi.nlm.nih.gov/pubmed/?term=3902...    2024  2024-07-18   \n",
      "1    https://www.ncbi.nlm.nih.gov/pubmed/?term=3901...    2024  2024-07-17   \n",
      "2    https://www.ncbi.nlm.nih.gov/pubmed/?term=3901...    2024  2024-07-17   \n",
      "3    https://www.ncbi.nlm.nih.gov/pubmed/?term=3899...    2024  2024-07-12   \n",
      "4    https://www.ncbi.nlm.nih.gov/pubmed/?term=3899...    2024  2024-07-11   \n",
      "..                                                 ...     ...         ...   \n",
      "295  https://www.ncbi.nlm.nih.gov/pubmed/?term=3642...    2022  2022-11-27   \n",
      "296  https://www.ncbi.nlm.nih.gov/pubmed/?term=3642...    2022  2022-11-25   \n",
      "297  https://www.ncbi.nlm.nih.gov/pubmed/?term=3642...    2022  2022-11-25   \n",
      "298  https://www.ncbi.nlm.nih.gov/pubmed/?term=3641...    2022  2022-11-25   \n",
      "299  https://www.ncbi.nlm.nih.gov/pubmed/?term=3640...    2022  2022-11-20   \n",
      "\n",
      "                                          meshHeadings  \n",
      "0                                                   []  \n",
      "1                                                   []  \n",
      "2                                                   []  \n",
      "3    [{'descriptor': 'Carcinoma, Hepatocellular'}, ...  \n",
      "4                                                   []  \n",
      "..                                                 ...  \n",
      "295  [{'descriptor': 'Humans'}, {'descriptor': 'Alg...  \n",
      "296                                                 []  \n",
      "297                                                 []  \n",
      "298  [{'descriptor': 'Humans'}, {'descriptor': 'Ade...  \n",
      "299  [{'descriptor': 'Humans'}, {'descriptor': 'Sev...  \n",
      "\n",
      "[300 rows x 9 columns] None\n",
      "<class 'pandas.core.frame.DataFrame'> ['authors', 'affiliations', 'title', 'abstract', 'pmid', 'url', 'pubYear', 'pubdate', 'meshHeadings']\n",
      "Reading dataframe...\n",
      "Computing tfidf...\n",
      "Outputting keywords...\n",
      "Updating matrix...\n",
      "kmeans\n",
      "Size of term_doc_mat: \n",
      "(1157, 300)\n",
      "Snippet of term_doc_mat: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "Non-zero values in term_doc_mat: 10427\n",
      "Size of term_term_mat: (1157, 1157)\n",
      "Extracted 227.0 coordinates\n",
      "Extracted 11.0 coordinates\n",
      "\n",
      "CENTROIDS\n",
      "----------\n",
      "[[0.7161194086074829, 0.0], [0.5609743595123291, 0.9999999403953552], [0.9701378345489502, 0.36516767740249634], [0.6808596849441528, 0.6504219770431519], [0.2558938264846802, 0.11689335107803345], [0.3358062505722046, 0.5826697945594788], [0.0, 0.44564518332481384], [1.0000001192092896, 0.8088444471359253], [0.5666813850402832, 0.3211309313774109], [0.15610551834106445, 0.8954044580459595]]\n",
      "\n",
      "CENTROID DISTANCES\n",
      "----------\n",
      "{0: {0: 0.0, 1: 1.011963372385311, 2: 0.4448289484029949, 3: 0.6513770001570034, 4: 0.47483854305071427, 5: 0.6958032679333662, 6: 0.8434611056859338, 7: 0.8572150240834603, 8: 0.3541988113686921, 9: 1.0561082808026394}, 1: {0: 1.011963372385311, 1: 0.0, 2: 0.7552660136936377, 3: 0.3695635854834303, 4: 0.9343186714007738, 5: 0.4741994600451014, 6: 0.7886706719011471, 7: 0.4788361309265929, 8: 0.6788929971250127, 9: 0.41816144427632906}, 2: {0: 0.4448289484029949, 1: 0.7552660136936377, 2: 0.0, 3: 0.40626575452806307, 4: 0.7561644293191653, 5: 0.670584617627199, 6: 0.9734701058495854, 7: 0.4446805955359704, 8: 0.4058526107475686, 9: 0.9714935179289255}, 3: {0: 0.6513770001570034, 1: 0.3695635854834303, 2: 0.40626575452806307, 3: 0.0, 4: 0.6820914715640591, 5: 0.35164219144348613, 6: 0.7109876551866522, 7: 0.3562980435158685, 8: 0.34852442801699723, 9: 0.5791229155935662}, 4: {0: 0.47483854305071427, 1: 0.9343186714007738, 2: 0.7561644293191653, 3: 0.6820914715640591, 4: 0.0, 5: 0.4725819408586921, 6: 0.41660462988136227, 7: 1.016115394139898, 8: 0.3718896284645446, 9: 0.7848804049755547}, 5: {0: 0.6958032679333662, 1: 0.4741994600451014, 2: 0.670584617627199, 3: 0.35164219144348613, 4: 0.4725819408586921, 5: 0.0, 6: 0.36268661680212755, 7: 0.7016469686410493, 8: 0.3488637336700704, 9: 0.36068729241603487}, 6: {0: 0.8434611056859338, 1: 0.7886706719011471, 2: 0.9734701058495854, 3: 0.7109876551866522, 4: 0.41660462988136227, 5: 0.36268661680212755, 6: 0.0, 7: 1.0639144437648766, 8: 0.5801996131411974, 9: 0.47608018027872506}, 7: {0: 0.8572150240834603, 1: 0.4788361309265929, 2: 0.4446805955359704, 3: 0.3562980435158685, 4: 1.016115394139898, 5: 0.7016469686410493, 6: 1.0639144437648766, 7: 0.0, 8: 0.6524029420805525, 9: 0.8483223048248149}, 8: {0: 0.3541988113686921, 1: 0.6788929971250127, 2: 0.4058526107475686, 3: 0.34852442801699723, 4: 0.3718896284645446, 5: 0.3488637336700704, 6: 0.5801996131411974, 7: 0.6524029420805525, 8: 0.0, 9: 0.7059480333198378}, 9: {0: 1.0561082808026394, 1: 0.41816144427632906, 2: 0.9714935179289255, 3: 0.5791229155935662, 4: 0.7848804049755547, 5: 0.36068729241603487, 6: 0.47608018027872506, 7: 0.8483223048248149, 8: 0.7059480333198378, 9: 0.0}}\n",
      "\n",
      "CLUSTER LABELS\n",
      "----------\n",
      "[['patients', 'median', 'group', 'study', 'groups', 'coronary', 'significantly', 'analysis', 'regression', 'follow-up', 'rate', 'adverse', 'higher', 'risk'], ['cell', 'expression', 'role', 'development', 'cells', 'mechanisms', 'protein', 'gene', 'therapeutic', 'potential', 'pathway', 'signaling', 'regulatory', 'proliferation', 'treatment'], ['training', 'model', 'dataset', 'learning', 'curve', 'features', 'radiomics', 'degs', 'predictive', 'machine', 'models', 'genes'], ['biodegradation', 'p450', 'pops', 'fungi', 'cytochrome', 'environmental', 'bacteria', 'communities', 'prospects', 'engineering', 'environments', 'microbial', 'mangrove', 'concern', 'enriched'], ['capacitive', 'deionization', 'storage', 'prepared', 'applications', 'materials', 'electrodes', 'electrode', 'fundamental', 'application', 'metal-organic', 'conductivity', 'preparation'], ['fiber', 'anti-resonant', 'hollow-core', 'core', 'wavelength', 'fibers', 'antiresonant', 'optical', 'spectroscopy', 'paper', 'error', 'noise', 'employing', 'experimentally'], ['metagenomic', 'mngs', 'next-generation', 'sequencing', 'balf', 'pathogens', 'suspected', 'pneumonia', 'infection', 'infections', 'antibiotic', 'concordance', 'samples', 'mixed', 'diagnosis'], ['adjusted', 'odds', 'ethnicity', 'multivariable', 'race', 'glaucoma', 'asian', 'population', 'california', 'medicare', 'individuals', 'beneficiaries'], ['study', 'high', 'significant', 'based', 'analysis', 'showed', 'treatment', 'research', 'china', 'increased', 'effects', 'development', 'risk', 'found', 'compared'], ['patient', 'case', 'site', 'tissue', 'hand', 'skin', 'rare', 'finger', 'examination', 'complications', 'salvage', 'fingers', 'limb', 'middle', 'motion']]\n",
      "\n",
      "CLUSTER ID & PUBLICATIONS\n",
      "----------\n",
      "{0: [0, 9, 15, 17, 36, 37, 44, 55, 57, 63, 64, 65, 69, 82, 87, 92, 95, 101, 103, 105, 117, 119, 139, 145, 150, 153, 157, 160, 168, 176, 192, 196, 206, 208, 210, 211, 217, 219, 222, 225, 230, 235, 237, 243, 253, 265, 266, 270, 274, 281, 282, 283, 285, 289, 292, 296, 299], 1: [3, 5, 11, 18, 19, 20, 21, 22, 26, 33, 41, 43, 45, 48, 51, 61, 73, 75, 76, 77, 80, 81, 84, 86, 88, 91, 94, 96, 97, 104, 106, 107, 115, 121, 135, 137, 141, 146, 148, 155, 158, 162, 164, 166, 167, 170, 172, 174, 175, 182, 185, 189, 190, 199, 200, 201, 209, 215, 223, 226, 227, 228, 229, 233, 234, 244, 250, 251, 254, 255, 258, 261, 269, 273, 275, 278, 279, 288, 290, 298], 2: [4, 7, 31, 83, 118, 124, 131, 147, 191, 194, 218, 224, 252, 259, 262], 3: [12, 16, 24, 38, 71, 100, 116, 122, 140, 181, 246], 4: [1, 14, 30, 47, 49, 54, 66, 78, 79, 89, 93, 110, 133, 152, 184, 186, 198, 232, 256, 263, 267, 284, 293, 294], 5: [28, 29, 58, 70, 72, 179, 187, 216, 220, 260, 272], 6: [52, 98, 111, 112, 136, 142, 149, 171, 193, 240, 271, 291], 7: [8, 68, 102, 113, 126, 132, 138, 144, 161, 177, 204, 207, 221, 238, 239, 245, 286, 287], 8: [6, 10, 23, 25, 27, 34, 39, 40, 42, 46, 53, 56, 59, 60, 62, 67, 74, 85, 99, 108, 109, 114, 120, 123, 127, 130, 134, 143, 151, 154, 156, 159, 169, 173, 178, 180, 183, 188, 197, 203, 205, 212, 213, 214, 231, 236, 241, 242, 247, 248, 249, 257, 264, 268, 276, 277, 280, 295], 9: [2, 13, 32, 35, 50, 90, 125, 128, 129, 163, 165, 195, 202, 297]}\n",
      "\n",
      "CLUSTER ID & PUBLICATION COUNT\n",
      "----------\n",
      "{0: 57, 1: 80, 2: 15, 3: 11, 4: 24, 5: 11, 6: 12, 7: 18, 8: 58, 9: 14}\n",
      "\n",
      "NETWORK\n",
      "----------\n",
      "[{'clusterID': 0, 'source': [0.7161194086074829, 0.0], 'target': [0.5609743595123291, 0.9999999403953552], 'distance': 'Distant'}, {'clusterID': 0, 'source': [0.7161194086074829, 0.0], 'target': [0.9701378345489502, 0.36516767740249634], 'distance': 'Very Similar'}, {'clusterID': 0, 'source': [0.7161194086074829, 0.0], 'target': [0.6808596849441528, 0.6504219770431519], 'distance': 'Similar'}, {'clusterID': 0, 'source': [0.7161194086074829, 0.0], 'target': [0.2558938264846802, 0.11689335107803345], 'distance': 'Very Similar'}, {'clusterID': 0, 'source': [0.7161194086074829, 0.0], 'target': [0.3358062505722046, 0.5826697945594788], 'distance': 'Similar'}, {'clusterID': 0, 'source': [0.7161194086074829, 0.0], 'target': [0.0, 0.44564518332481384], 'distance': 'Distant'}, {'clusterID': 0, 'source': [0.7161194086074829, 0.0], 'target': [1.0000001192092896, 0.8088444471359253], 'distance': 'Distant'}, {'clusterID': 0, 'source': [0.7161194086074829, 0.0], 'target': [0.5666813850402832, 0.3211309313774109], 'distance': 'Very Similar'}, {'clusterID': 0, 'source': [0.7161194086074829, 0.0], 'target': [0.15610551834106445, 0.8954044580459595], 'distance': 'Distant'}, {'clusterID': 1, 'source': [0.5609743595123291, 0.9999999403953552], 'target': [0.7161194086074829, 0.0], 'distance': 'Distant'}, {'clusterID': 1, 'source': [0.5609743595123291, 0.9999999403953552], 'target': [0.9701378345489502, 0.36516767740249634], 'distance': 'Distant'}, {'clusterID': 1, 'source': [0.5609743595123291, 0.9999999403953552], 'target': [0.6808596849441528, 0.6504219770431519], 'distance': 'Very Similar'}, {'clusterID': 1, 'source': [0.5609743595123291, 0.9999999403953552], 'target': [0.2558938264846802, 0.11689335107803345], 'distance': 'Distant'}, {'clusterID': 1, 'source': [0.5609743595123291, 0.9999999403953552], 'target': [0.3358062505722046, 0.5826697945594788], 'distance': 'Very Similar'}, {'clusterID': 1, 'source': [0.5609743595123291, 0.9999999403953552], 'target': [0.0, 0.44564518332481384], 'distance': 'Distant'}, {'clusterID': 1, 'source': [0.5609743595123291, 0.9999999403953552], 'target': [1.0000001192092896, 0.8088444471359253], 'distance': 'Very Similar'}, {'clusterID': 1, 'source': [0.5609743595123291, 0.9999999403953552], 'target': [0.5666813850402832, 0.3211309313774109], 'distance': 'Similar'}, {'clusterID': 1, 'source': [0.5609743595123291, 0.9999999403953552], 'target': [0.15610551834106445, 0.8954044580459595], 'distance': 'Very Similar'}, {'clusterID': 2, 'source': [0.9701378345489502, 0.36516767740249634], 'target': [0.7161194086074829, 0.0], 'distance': 'Very Similar'}, {'clusterID': 2, 'source': [0.9701378345489502, 0.36516767740249634], 'target': [0.5609743595123291, 0.9999999403953552], 'distance': 'Distant'}, {'clusterID': 2, 'source': [0.9701378345489502, 0.36516767740249634], 'target': [0.6808596849441528, 0.6504219770431519], 'distance': 'Very Similar'}, {'clusterID': 2, 'source': [0.9701378345489502, 0.36516767740249634], 'target': [0.2558938264846802, 0.11689335107803345], 'distance': 'Distant'}, {'clusterID': 2, 'source': [0.9701378345489502, 0.36516767740249634], 'target': [0.3358062505722046, 0.5826697945594788], 'distance': 'Similar'}, {'clusterID': 2, 'source': [0.9701378345489502, 0.36516767740249634], 'target': [0.0, 0.44564518332481384], 'distance': 'Distant'}, {'clusterID': 2, 'source': [0.9701378345489502, 0.36516767740249634], 'target': [1.0000001192092896, 0.8088444471359253], 'distance': 'Very Similar'}, {'clusterID': 2, 'source': [0.9701378345489502, 0.36516767740249634], 'target': [0.5666813850402832, 0.3211309313774109], 'distance': 'Very Similar'}, {'clusterID': 2, 'source': [0.9701378345489502, 0.36516767740249634], 'target': [0.15610551834106445, 0.8954044580459595], 'distance': 'Distant'}, {'clusterID': 3, 'source': [0.6808596849441528, 0.6504219770431519], 'target': [0.7161194086074829, 0.0], 'distance': 'Similar'}, {'clusterID': 3, 'source': [0.6808596849441528, 0.6504219770431519], 'target': [0.5609743595123291, 0.9999999403953552], 'distance': 'Very Similar'}, {'clusterID': 3, 'source': [0.6808596849441528, 0.6504219770431519], 'target': [0.9701378345489502, 0.36516767740249634], 'distance': 'Very Similar'}, {'clusterID': 3, 'source': [0.6808596849441528, 0.6504219770431519], 'target': [0.2558938264846802, 0.11689335107803345], 'distance': 'Similar'}, {'clusterID': 3, 'source': [0.6808596849441528, 0.6504219770431519], 'target': [0.3358062505722046, 0.5826697945594788], 'distance': 'Very Similar'}, {'clusterID': 3, 'source': [0.6808596849441528, 0.6504219770431519], 'target': [0.0, 0.44564518332481384], 'distance': 'Similar'}, {'clusterID': 3, 'source': [0.6808596849441528, 0.6504219770431519], 'target': [1.0000001192092896, 0.8088444471359253], 'distance': 'Very Similar'}, {'clusterID': 3, 'source': [0.6808596849441528, 0.6504219770431519], 'target': [0.5666813850402832, 0.3211309313774109], 'distance': 'Very Similar'}, {'clusterID': 3, 'source': [0.6808596849441528, 0.6504219770431519], 'target': [0.15610551834106445, 0.8954044580459595], 'distance': 'Similar'}, {'clusterID': 4, 'source': [0.2558938264846802, 0.11689335107803345], 'target': [0.7161194086074829, 0.0], 'distance': 'Very Similar'}, {'clusterID': 4, 'source': [0.2558938264846802, 0.11689335107803345], 'target': [0.5609743595123291, 0.9999999403953552], 'distance': 'Distant'}, {'clusterID': 4, 'source': [0.2558938264846802, 0.11689335107803345], 'target': [0.9701378345489502, 0.36516767740249634], 'distance': 'Distant'}, {'clusterID': 4, 'source': [0.2558938264846802, 0.11689335107803345], 'target': [0.6808596849441528, 0.6504219770431519], 'distance': 'Similar'}, {'clusterID': 4, 'source': [0.2558938264846802, 0.11689335107803345], 'target': [0.3358062505722046, 0.5826697945594788], 'distance': 'Very Similar'}, {'clusterID': 4, 'source': [0.2558938264846802, 0.11689335107803345], 'target': [0.0, 0.44564518332481384], 'distance': 'Very Similar'}, {'clusterID': 4, 'source': [0.2558938264846802, 0.11689335107803345], 'target': [1.0000001192092896, 0.8088444471359253], 'distance': 'Distant'}, {'clusterID': 4, 'source': [0.2558938264846802, 0.11689335107803345], 'target': [0.5666813850402832, 0.3211309313774109], 'distance': 'Very Similar'}, {'clusterID': 4, 'source': [0.2558938264846802, 0.11689335107803345], 'target': [0.15610551834106445, 0.8954044580459595], 'distance': 'Distant'}, {'clusterID': 5, 'source': [0.3358062505722046, 0.5826697945594788], 'target': [0.7161194086074829, 0.0], 'distance': 'Similar'}, {'clusterID': 5, 'source': [0.3358062505722046, 0.5826697945594788], 'target': [0.5609743595123291, 0.9999999403953552], 'distance': 'Very Similar'}, {'clusterID': 5, 'source': [0.3358062505722046, 0.5826697945594788], 'target': [0.9701378345489502, 0.36516767740249634], 'distance': 'Similar'}, {'clusterID': 5, 'source': [0.3358062505722046, 0.5826697945594788], 'target': [0.6808596849441528, 0.6504219770431519], 'distance': 'Very Similar'}, {'clusterID': 5, 'source': [0.3358062505722046, 0.5826697945594788], 'target': [0.2558938264846802, 0.11689335107803345], 'distance': 'Very Similar'}, {'clusterID': 5, 'source': [0.3358062505722046, 0.5826697945594788], 'target': [0.0, 0.44564518332481384], 'distance': 'Very Similar'}, {'clusterID': 5, 'source': [0.3358062505722046, 0.5826697945594788], 'target': [1.0000001192092896, 0.8088444471359253], 'distance': 'Similar'}, {'clusterID': 5, 'source': [0.3358062505722046, 0.5826697945594788], 'target': [0.5666813850402832, 0.3211309313774109], 'distance': 'Very Similar'}, {'clusterID': 5, 'source': [0.3358062505722046, 0.5826697945594788], 'target': [0.15610551834106445, 0.8954044580459595], 'distance': 'Very Similar'}, {'clusterID': 6, 'source': [0.0, 0.44564518332481384], 'target': [0.7161194086074829, 0.0], 'distance': 'Distant'}, {'clusterID': 6, 'source': [0.0, 0.44564518332481384], 'target': [0.5609743595123291, 0.9999999403953552], 'distance': 'Distant'}, {'clusterID': 6, 'source': [0.0, 0.44564518332481384], 'target': [0.9701378345489502, 0.36516767740249634], 'distance': 'Distant'}, {'clusterID': 6, 'source': [0.0, 0.44564518332481384], 'target': [0.6808596849441528, 0.6504219770431519], 'distance': 'Similar'}, {'clusterID': 6, 'source': [0.0, 0.44564518332481384], 'target': [0.2558938264846802, 0.11689335107803345], 'distance': 'Very Similar'}, {'clusterID': 6, 'source': [0.0, 0.44564518332481384], 'target': [0.3358062505722046, 0.5826697945594788], 'distance': 'Very Similar'}, {'clusterID': 6, 'source': [0.0, 0.44564518332481384], 'target': [1.0000001192092896, 0.8088444471359253], 'distance': 'Distant'}, {'clusterID': 6, 'source': [0.0, 0.44564518332481384], 'target': [0.5666813850402832, 0.3211309313774109], 'distance': 'Similar'}, {'clusterID': 6, 'source': [0.0, 0.44564518332481384], 'target': [0.15610551834106445, 0.8954044580459595], 'distance': 'Very Similar'}, {'clusterID': 7, 'source': [1.0000001192092896, 0.8088444471359253], 'target': [0.7161194086074829, 0.0], 'distance': 'Distant'}, {'clusterID': 7, 'source': [1.0000001192092896, 0.8088444471359253], 'target': [0.5609743595123291, 0.9999999403953552], 'distance': 'Very Similar'}, {'clusterID': 7, 'source': [1.0000001192092896, 0.8088444471359253], 'target': [0.9701378345489502, 0.36516767740249634], 'distance': 'Very Similar'}, {'clusterID': 7, 'source': [1.0000001192092896, 0.8088444471359253], 'target': [0.6808596849441528, 0.6504219770431519], 'distance': 'Very Similar'}, {'clusterID': 7, 'source': [1.0000001192092896, 0.8088444471359253], 'target': [0.2558938264846802, 0.11689335107803345], 'distance': 'Distant'}, {'clusterID': 7, 'source': [1.0000001192092896, 0.8088444471359253], 'target': [0.3358062505722046, 0.5826697945594788], 'distance': 'Similar'}, {'clusterID': 7, 'source': [1.0000001192092896, 0.8088444471359253], 'target': [0.0, 0.44564518332481384], 'distance': 'Distant'}, {'clusterID': 7, 'source': [1.0000001192092896, 0.8088444471359253], 'target': [0.5666813850402832, 0.3211309313774109], 'distance': 'Similar'}, {'clusterID': 7, 'source': [1.0000001192092896, 0.8088444471359253], 'target': [0.15610551834106445, 0.8954044580459595], 'distance': 'Distant'}, {'clusterID': 8, 'source': [0.5666813850402832, 0.3211309313774109], 'target': [0.7161194086074829, 0.0], 'distance': 'Very Similar'}, {'clusterID': 8, 'source': [0.5666813850402832, 0.3211309313774109], 'target': [0.5609743595123291, 0.9999999403953552], 'distance': 'Similar'}, {'clusterID': 8, 'source': [0.5666813850402832, 0.3211309313774109], 'target': [0.9701378345489502, 0.36516767740249634], 'distance': 'Very Similar'}, {'clusterID': 8, 'source': [0.5666813850402832, 0.3211309313774109], 'target': [0.6808596849441528, 0.6504219770431519], 'distance': 'Very Similar'}, {'clusterID': 8, 'source': [0.5666813850402832, 0.3211309313774109], 'target': [0.2558938264846802, 0.11689335107803345], 'distance': 'Very Similar'}, {'clusterID': 8, 'source': [0.5666813850402832, 0.3211309313774109], 'target': [0.3358062505722046, 0.5826697945594788], 'distance': 'Very Similar'}, {'clusterID': 8, 'source': [0.5666813850402832, 0.3211309313774109], 'target': [0.0, 0.44564518332481384], 'distance': 'Similar'}, {'clusterID': 8, 'source': [0.5666813850402832, 0.3211309313774109], 'target': [1.0000001192092896, 0.8088444471359253], 'distance': 'Similar'}, {'clusterID': 8, 'source': [0.5666813850402832, 0.3211309313774109], 'target': [0.15610551834106445, 0.8954044580459595], 'distance': 'Similar'}, {'clusterID': 9, 'source': [0.15610551834106445, 0.8954044580459595], 'target': [0.7161194086074829, 0.0], 'distance': 'Distant'}, {'clusterID': 9, 'source': [0.15610551834106445, 0.8954044580459595], 'target': [0.5609743595123291, 0.9999999403953552], 'distance': 'Very Similar'}, {'clusterID': 9, 'source': [0.15610551834106445, 0.8954044580459595], 'target': [0.9701378345489502, 0.36516767740249634], 'distance': 'Distant'}, {'clusterID': 9, 'source': [0.15610551834106445, 0.8954044580459595], 'target': [0.6808596849441528, 0.6504219770431519], 'distance': 'Similar'}, {'clusterID': 9, 'source': [0.15610551834106445, 0.8954044580459595], 'target': [0.2558938264846802, 0.11689335107803345], 'distance': 'Distant'}, {'clusterID': 9, 'source': [0.15610551834106445, 0.8954044580459595], 'target': [0.3358062505722046, 0.5826697945594788], 'distance': 'Very Similar'}, {'clusterID': 9, 'source': [0.15610551834106445, 0.8954044580459595], 'target': [0.0, 0.44564518332481384], 'distance': 'Very Similar'}, {'clusterID': 9, 'source': [0.15610551834106445, 0.8954044580459595], 'target': [1.0000001192092896, 0.8088444471359253], 'distance': 'Distant'}, {'clusterID': 9, 'source': [0.15610551834106445, 0.8954044580459595], 'target': [0.5666813850402832, 0.3211309313774109], 'distance': 'Similar'}]\n",
      "exit cluster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aravind/anaconda3/envs/PATTIE_v2/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1162: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def cluster(raw_query_inp, num_cls_inp):\n",
    "    print(\"cluster\")\n",
    "    error = None\n",
    "    dataset = \"PubMedAPI\"\n",
    "\n",
    "    field = \"All fields\"\n",
    "    raw_query = raw_query_inp\n",
    "    entity = \"keywords\"\n",
    "    q = raw_query\n",
    "    # print(q)\n",
    "    num_cls = num_cls_inp\n",
    "\n",
    "    if dataset == \"Placeholder\":\n",
    "        return redirect(url_for(\"home\"))\n",
    "    \n",
    "    # dynamic dataset\n",
    "    elif dataset == \"PubMedAPI\":\n",
    "        print(\"PubMedAPI in routes.py\")\n",
    "        import cluster_pymedAPI\n",
    "        import es_search\n",
    "\n",
    "        # print(\"start_date\")\n",
    "        # print(start_date)\n",
    "        num_cls = int(num_cls)\n",
    "        # try:\n",
    "        # data, error = es_search.retrieve(q, entity)\n",
    "        data, error = cluster_pymedAPI.retrieve(q)\n",
    "        print(data, error)\n",
    "        print(type(data), data.columns.tolist())\n",
    "        if error == None:\n",
    "            if entity == \"authors\":\n",
    "                # doc_term_mat, df, w2id, bibs = vl.read_df_authors(\n",
    "                #     data, dataset, stopwords)\n",
    "                (\n",
    "                    orig_doc_term_mat,\n",
    "                    orig_df,\n",
    "                    df,\n",
    "                    dfr,\n",
    "                    keywords,\n",
    "                    cluster_centers,\n",
    "                    cluster_desc,\n",
    "                    coordinates,\n",
    "                    id2members,\n",
    "                    bibs,\n",
    "                    org_ids,\n",
    "                    author_names,\n",
    "                    author_docs,\n",
    "                ) = vl.process_authors_4(data, dataset, stopwords)\n",
    "            else:\n",
    "                doc_term_mat, df, w2id, bibs = vl.read_df(data, dataset, stopwords)\n",
    "        else:\n",
    "            # flash(error)\n",
    "            return redirect(url_for(\"home\"))\n",
    "\n",
    "    # print(\"doc_term_mat\")\n",
    "    # print(doc_term_mat)\n",
    "    # print(df)\n",
    "\n",
    "    if dataset != \"Experts\" and len(bibs) < 10:\n",
    "        #       flash('No clusters found. Broaden your search.')\n",
    "        flash(\n",
    "            \"Please upload larger files or search for more relevant keywords to get enough results.\"\n",
    "        )\n",
    "        #        return redirect(url_for('home'))\n",
    "        return redirect(request.referrer)\n",
    "\n",
    "    if entity != \"authors\" and dataset != \"Experts\":\n",
    "        # Remove terms whose df is lower than mindf\n",
    "        if MINDF > 0:\n",
    "            inf = []\n",
    "            for w in df:\n",
    "                if df[w] <= MINDF:\n",
    "                    inf.append(w)\n",
    "            for w in inf:\n",
    "                del df[w]\n",
    "\n",
    "        # Save org data\n",
    "        orig_doc_term_mat = doc_term_mat\n",
    "        orig_df = df\n",
    "\n",
    "        # Compute tfidf and find key terms\n",
    "        # print(\"Computing TFIDF and finding key terms...\")\n",
    "        if dataset == \"NYTIMES\" or dataset == \"PLOS\" or dataset == \"DIABETES\":\n",
    "            doc_term_mat, dfr = vl.compute_tfidf(doc_term_mat, df, rank=10)\n",
    "        else:\n",
    "            doc_term_mat, dfr = vl.compute_tfidf(doc_term_mat, df, rank=30)\n",
    "\n",
    "        # Sort and output results (discovered keywords)\n",
    "        keywords = vl.output_keywords(len(doc_term_mat), dfr, df, p_docs=1.0)\n",
    "        # print('Keywords...')\n",
    "        # print(keywords)\n",
    "\n",
    "        # Create new matrix with the keywords\n",
    "        doc_term_mat, org_ids = vl.update(doc_term_mat, keywords)\n",
    "\n",
    "        # Convert to sparse matrix\n",
    "        doc_term_mat = vl.convert_sparse(doc_term_mat, keywords)\n",
    "\n",
    "        # Clustering\n",
    "        # print()\n",
    "        # print(\"Clustering...\")\n",
    "\n",
    "        # n_components: number of dimensions for LSA\n",
    "        # k: number of clusters\n",
    "        # n_desc: number of keywords (desc) for each cluster\n",
    "\n",
    "        if dataset == \"PLOS\":\n",
    "            id2members, cluster_centers, cluster_desc, coordinates, error = vl.kmeans(\n",
    "                doc_term_mat, keywords, org_ids, n_components=50, k=num_cls, n_desc=25\n",
    "            )\n",
    "        else:\n",
    "            id2members, cluster_centers, cluster_desc, coordinates, error = vl.kmeans(\n",
    "                doc_term_mat, keywords, org_ids, n_components=20, k=num_cls, n_desc=15\n",
    "            )\n",
    "\n",
    "        if error != None:  # needs to be changed\n",
    "            print(error)\n",
    "            flash(\n",
    "                \"Sorry, the entity you've chosen does not generate meaningful clusters for current query. Please try other entities or queries. \"\n",
    "            )\n",
    "            return redirect(request.referrer)\n",
    "\n",
    "    # get cluster colors\n",
    "    \"\"\"\n",
    "    # center is (.5,0)\n",
    "    cc = np.array(cluster_centers)-[.5,0]\n",
    "    hue = ((np.arctan2(cc[:,1],cc[:,0])/np.pi+1)*180*2).\\\n",
    "        astype(int).tolist()\n",
    "    satr = (np.linalg.norm(cc, axis=1)/math.sqrt(5)*2).tolist()\n",
    "    \"\"\"\n",
    "    # center is (.5,.5)\n",
    "    cc = np.array(cluster_centers) - 0.5\n",
    "    hue = ((np.arctan2(cc[:, 1], cc[:, 0]) / np.pi + 1) * 180).astype(int).tolist()\n",
    "    satr = (np.linalg.norm(cc, axis=1) / math.sqrt(2) * 2).tolist()\n",
    "    val = [0.8] * num_cls\n",
    "\n",
    "    # Create data to pass to result.html\n",
    "    df_new = dict()\n",
    "    dfr_new = dict()\n",
    "    for w in keywords:\n",
    "        df_new[w] = df[w]\n",
    "        dfr_new[w] = dfr[w]\n",
    "\n",
    "    # create adjacency matrix that will be used for network edges\n",
    "    centroid_matrix = pd.DataFrame.from_records(cluster_centers)\n",
    "    centroid_distances = pd.DataFrame(\n",
    "        squareform(pdist(centroid_matrix, metric=\"euclidean\")),\n",
    "        columns=list(id2members.keys()),\n",
    "        index=list(id2members.keys()),\n",
    "    ).to_dict()\n",
    "\n",
    "    id2freq = {x: len(id2members[x]) for x in id2members}\n",
    "\n",
    "    # create network data structure\n",
    "    # print('\\nEUCLIDEAN DISTANCES\\n----------')\n",
    "    edges = []\n",
    "    for i in id2members:\n",
    "        for k, v in centroid_distances[i].items():\n",
    "            # print(i, ' to ', k, ' = ', v)\n",
    "            if i != k:\n",
    "                if v > 0.75:\n",
    "                    v = \"Distant\"\n",
    "                    edge = {\n",
    "                        \"clusterID\": i,\n",
    "                        \"source\": cluster_centers[i],\n",
    "                        \"target\": cluster_centers[k],\n",
    "                        \"distance\": v,\n",
    "                    }\n",
    "                    edges.append(edge)\n",
    "                elif v <= 0.75 and v > 0.50:\n",
    "                    v = \"Similar\"\n",
    "                    edge = {\n",
    "                        \"clusterID\": i,\n",
    "                        \"source\": cluster_centers[i],\n",
    "                        \"target\": cluster_centers[k],\n",
    "                        \"distance\": v,\n",
    "                    }\n",
    "                    edges.append(edge)\n",
    "                elif v <= 0.50:\n",
    "                    v = \"Very Similar\"\n",
    "                    edge = {\n",
    "                        \"clusterID\": i,\n",
    "                        \"source\": cluster_centers[i],\n",
    "                        \"target\": cluster_centers[k],\n",
    "                        \"distance\": v,\n",
    "                    }\n",
    "                    edges.append(edge)\n",
    "\n",
    "                # elif v >= 0.50:\n",
    "                #     v = \"Not Similar\"\n",
    "                # edge = {\"clusterID\":i,\"source\":cluster_centers[i],\"target\":cluster_centers[k],\"distance\":v}\n",
    "                # edges.append(edge)\n",
    "    # print('\\nBIBLIOGRAPHY\\n----------')\n",
    "    sources = []\n",
    "    id2meshTerms = {}\n",
    "    if dataset == \"PubMedAPI\":\n",
    "        from collections import Counter\n",
    "\n",
    "        for cluster_id in sorted(id2members.keys()):\n",
    "            concepts = cluster_desc[cluster_id]\n",
    "            references = []\n",
    "            for bib_id in id2members[cluster_id]:\n",
    "                references.append(bibs[bib_id])\n",
    "            for paper in references:\n",
    "                if type(paper[\"author\"]) != str and None in paper[\"author\"]:\n",
    "                    paper[\"author\"] = [i for i in paper[\"author\"] if i]\n",
    "            bibliography = {\"concepts\": concepts, \"references\": references}\n",
    "            meshTerms = []\n",
    "            for ref in references:\n",
    "                meshHeadingList = ref.get(\"meshHeadings\")  # ref[\"meshHeadings\"]\n",
    "                for mesh in meshHeadingList:\n",
    "                    meshTerms.append(mesh[\"descriptor\"])\n",
    "            # print('concept: {}'.format(concepts))\n",
    "            # print('meshTerms: {}'.format(Counter(meshTerms)))\n",
    "            # print('\\n')\n",
    "            id2meshTerms[cluster_id] = str(dict(Counter(meshTerms).most_common(20)))\n",
    "            # vl.genes_coverage(concepts, references)\n",
    "            sources.append(bibliography)\n",
    "        # print(sources)\n",
    "\n",
    "    \"\"\"\n",
    "    cnt = np.unique(membership, return_counts=True)\n",
    "    keys = [x for x in cnt[0]]\n",
    "    values = [int(x) for x in cnt[1]]\n",
    "    id2freq = dict(zip(keys, values))\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # Prepare bib data if num of documents is small\n",
    "    if len(org_ids) > 150:\n",
    "        bibs = []\n",
    "    \"\"\"\n",
    "\n",
    "    # session['doc_term_mat'] = doc_term_mat  # term-doc matrix\n",
    "    session = {}\n",
    "    session[\"docs_org\"] = orig_doc_term_mat  # doc-term raw freq matrix\n",
    "    session[\"df_org\"] = orig_df\n",
    "    session[\"num_cls\"] = num_cls\n",
    "    session[\"id2members_0\"] = id2members\n",
    "    session[\"cluster_desc_0\"] = cluster_desc\n",
    "    session[\"xy_0\"] = cluster_centers\n",
    "    # session[\"state\"] = state\n",
    "    session[\"hue_0\"] = hue\n",
    "    session[\"satr_0\"] = satr\n",
    "    session[\"org_ids_0\"] = org_ids\n",
    "    session[\"bibs_0\"] = bibs\n",
    "    session[\"dataset\"] = dataset\n",
    "    session[\"edges_0\"] = edges\n",
    "    session[\"sources\"] = sources\n",
    "    session[\"id2meshTerms\"] = id2meshTerms\n",
    "\n",
    "    session[\"entity\"] = entity\n",
    "    if entity == \"authors\":\n",
    "        session[\"author_names\"] = author_names\n",
    "        session[\"author_docs\"] = author_docs\n",
    "        \n",
    "    # Convert NumPy arrays to lists\n",
    "    def convert_ndarray_to_list(obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_ndarray_to_list(item) for item in obj]\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: convert_ndarray_to_list(value) for key, value in obj.items()}\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    # Ensure orig_doc_term_mat is a list\n",
    "    orig_doc_term_mat_list = convert_ndarray_to_list(orig_doc_term_mat)\n",
    "    # Ensure orig_df is a list or dict (depending on its original type)\n",
    "    orig_df_list = convert_ndarray_to_list(orig_df)\n",
    "    # Ensure cluster_centers is a list\n",
    "    cluster_centers_list = convert_ndarray_to_list(cluster_centers)\n",
    "    # Ensure coordinates is a list\n",
    "    coordinates_list = convert_ndarray_to_list(coordinates)\n",
    "\n",
    "\n",
    "    sessionData = {\n",
    "        \"docs_org\": convert_ndarray_to_list(orig_doc_term_mat),  # doc-term raw freq matrix\n",
    "        \"df_org\": convert_ndarray_to_list(orig_df),\n",
    "        \"num_cls\": num_cls,\n",
    "        \"id2members_0\": convert_ndarray_to_list(id2members),\n",
    "        \"cluster_desc_0\": convert_ndarray_to_list(cluster_desc),\n",
    "        \"xy_0\": convert_ndarray_to_list(cluster_centers),\n",
    "        # \"state\": state,\n",
    "        \"hue_0\": convert_ndarray_to_list(hue),\n",
    "        \"satr_0\": convert_ndarray_to_list(satr),\n",
    "        \"org_ids_0\": convert_ndarray_to_list(org_ids),\n",
    "        \"dataset\": dataset,\n",
    "        \"edges_0\": convert_ndarray_to_list(edges),\n",
    "        \"sources\": convert_ndarray_to_list(sources)\n",
    "    }\n",
    "\n",
    "    # session['df'] = df_new\n",
    "    # session['dfr'] = dfr_new\n",
    "    # session['keywords'] = keywords\n",
    "\n",
    "    # print(\"bibs\")\n",
    "    # print(bibs)\n",
    "\n",
    "    # output the data to terminal\n",
    "    print('\\nCENTROIDS\\n----------')\n",
    "    print(cluster_centers)\n",
    "    print('\\nCENTROID DISTANCES\\n----------')\n",
    "    print(centroid_distances)\n",
    "    print('\\nCLUSTER LABELS\\n----------')\n",
    "    print(cluster_desc)\n",
    "    print('\\nCLUSTER ID & PUBLICATIONS\\n----------')\n",
    "    print(id2members)\n",
    "    print('\\nCLUSTER ID & PUBLICATION COUNT\\n----------')\n",
    "    print(id2freq)\n",
    "    print('\\nNETWORK\\n----------')\n",
    "    print(edges)\n",
    "\n",
    "\n",
    "\n",
    "    if entity == \"authors\" or entity == \"experts\" or entity == \"topics\":\n",
    "        # to tell the difference between keyword search and author search so that sessionStorage won't keep old data when switching between them\n",
    "        q = q + \"_\" + entity\n",
    "\n",
    "    print(\"exit cluster\")\n",
    "    \n",
    "cluster(\"Fei Yu\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PATTIE_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
